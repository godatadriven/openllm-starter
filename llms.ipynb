{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open source LLM kickstarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load open source models\n",
    "\n",
    "Check out the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), or chat with some open source models [here](https://chat.lmsys.org/) for inspiration. We've chosen a GPU to allow some of the bigger models to run, but if you want to go really big, you may need to refer to some tricks like [8-bit quantization](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/HuggingFace_int8_demo.ipynb), or model parallellism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/MBZUAI/LaMini-T5-738M\n",
    "lamini = pipeline(\n",
    "    'text2text-generation', \n",
    "    model=\"MBZUAI/LaMini-T5-738M\", \n",
    "    max_length=512, \n",
    "    do_sample=True, \n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/databricks/dolly-v2-3b\n",
    "dolly = pipeline(\n",
    "    'text2text-generation', \n",
    "    model=\"databricks/dolly-v2-3b\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True, \n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've added device=0 to make sure we're using the GPU\n",
    "# we can double check this as well by running the following\n",
    "dolly.device, lamini.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "Let's see if they're doing what we expect them to do.\n",
    "Be creative, play around, have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's so neat about open source large language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamini(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
